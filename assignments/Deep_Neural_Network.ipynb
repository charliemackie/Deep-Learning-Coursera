{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-Neural-Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmgtlWv9i1dcJ9SdEaqqYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charliemackie/Deep-Learning-Coursera/blob/master/Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8O_ExnBBKYb",
        "colab_type": "text"
      },
      "source": [
        "# Step by Step Deep Neural Network - Charlie Mackie\n",
        "\n",
        "Implementation of DNN functions for 2-layer and L-layer networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ig5JI_VCPRA",
        "colab_type": "text"
      },
      "source": [
        "Import necessary packages \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sbyShatCSqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # default photo size\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOC24GDDWcw",
        "colab_type": "text"
      },
      "source": [
        "DNN utils to be used (Activation Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTwf4je7DcYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXEi0jSXETA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X29fpsSgEZp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLlAifgsE0Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G84YAb8uE5tX",
        "colab_type": "text"
      },
      "source": [
        "Assignment outline: \n",
        "\n",
        "(Will be making a 2-layer network and an L-layer network)\n",
        "\n",
        "1. Initialize the parameters for a Deep 'L' Layer network\n",
        "\n",
        "2. Implement the forward propagation Module\n",
        "\n",
        "  - Linear portion to calculate Z[L]\n",
        "  - Compute the ReLu of Z[L] = a[L]\n",
        "  - Stack this piece and use a sigmoid activation at the final layer\n",
        "\n",
        "3. Compute the Loss\n",
        "\n",
        "4. Implement backward propagation\n",
        "\n",
        "5. Update the paramaters (Optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_igFSDXL5Kj",
        "colab_type": "text"
      },
      "source": [
        "# Initialize Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbIL_62FGQin",
        "colab_type": "text"
      },
      "source": [
        "2-layer network: Initialize the parameters of our network (W and b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZQxGfjGHjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "  \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "  W1 = np.random.randn(n_h, n_x) * 0.01 # shape = (hidden units of layer 1, input layer units)\n",
        "  b1 = np.zeros((n_h, 1)) # shape = (hidden units, 1)\n",
        "  W2 = np.random.randn(n_y, n_h) * 0.01 # shape = (units of output layer, hidden units of layer 1)\n",
        "  b2 = np.zeros((n_y,1)) # shape = (units of output layer, 1)\n",
        "\n",
        "  # ensure the shapes are the right size \n",
        "  assert(W1.shape == (n_h, n_x))\n",
        "  assert(b1.shape == (n_h, 1))\n",
        "  assert(W2.shape == (n_y, n_h))\n",
        "  assert(b2.shape == (n_y, 1))\n",
        "\n",
        "  # store the parameters in a dictionary that can be easily accessed\n",
        "  parameters = {\"W1\" : W1,\n",
        "                \"b1\" : b1,\n",
        "                \"W2\" : W2,\n",
        "                \"b2\" : b2,}\n",
        "\n",
        "  return parameters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QUNP2p1I8Ld",
        "colab_type": "code",
        "outputId": "d77e6637-2383-448c-af22-6272d9cefd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# test case\n",
        "parameters = initialize_parameters(3,2,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.0053004   0.0040213  -0.00188167]\n",
            " [ 0.00387259 -0.00817396  0.00242415]]\n",
            "b1 = [[0.]\n",
            " [0.]]\n",
            "W2 = [[0.00168808 0.00092437]]\n",
            "b2 = [[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcIhJhVtJc7p",
        "colab_type": "text"
      },
      "source": [
        "L-layer network: initialize parameters\n",
        "\n",
        "  - need to ensure that we have the right sized matricies for linear forward prop\n",
        "  - Python will broadcast our b value \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM6sfnLDJjfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1,L):\n",
        "      parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
        "      parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "      \n",
        "      # assert shapes \n",
        "      assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "      assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoynP3Y9LO9X",
        "colab_type": "code",
        "outputId": "921adfae-2ea4-40f3-cef6-71428f9e5c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# test case\n",
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.19945411  0.43036863 -1.84990859 -0.26587459 -1.11555909]\n",
            " [ 0.00893857  0.72543373  0.33715813 -0.02690619 -1.34955181]\n",
            " [ 0.30319862  1.83884901 -0.03927289 -2.66154231  0.78309335]\n",
            " [-1.37364701  1.4917113  -0.12029932  1.44032285 -0.00421307]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[-0.38632681  0.93274408  1.20165047  0.38819642]\n",
            " [ 1.40203533 -0.02939617 -1.16571705  1.7339532 ]\n",
            " [ 0.93033679 -0.52792869 -2.15863711 -0.19076963]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuM-NwpZMDNo",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO2gswvJMK7N",
        "colab_type": "text"
      },
      "source": [
        "2-layer network: Linear Forward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzWp0vCbMztm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEibYRXVNbt5",
        "colab_type": "code",
        "outputId": "3957dd80-3b5b-4d36-924f-4f37ae67fe4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test case\n",
        "A = np.random.randn(3,2)\n",
        "W = np.random.randn(1,3)\n",
        "b = np.random.randn(1,1)\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z = [[ 2.21141706 -1.36463312]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Eo60LDN5m3",
        "colab_type": "text"
      },
      "source": [
        "2-layer network: Linear Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-h6CgVN8Gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    if activation == \"sigmoid\":\n",
        "      Z, linear_cache = linear_forward(A_prev, W , b)\n",
        "      A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    if activation == 'relu':\n",
        "      Z, linear_cache = linear_forward(A_prev, W , b)\n",
        "      A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB-I02bzOsLU",
        "colab_type": "code",
        "outputId": "0ec34970-a3a4-41b6-a7b7-e9c1859bf514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test case \n",
        "A_prev = np.random.randn(3,2)\n",
        "W = np.random.randn(1,3)\n",
        "b = np.random.randn(1,1)\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With sigmoid: A = [[0.45891306 0.02786052]]\n",
            "With ReLU: A = [[0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SynBKVY_PL1S",
        "colab_type": "text"
      },
      "source": [
        "L-layer network: forward linear + activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd46v7TlPTQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) //2\n",
        "\n",
        "    # Linear -> Relu\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Linear -> Sigmoid\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfB9bmR9QmnH",
        "colab_type": "code",
        "outputId": "2ce4a0cd-16b6-4c76-d2da-ea70910801e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test case\n",
        "\n",
        "X = np.random.randn(5,4)\n",
        "W1 = np.random.randn(4,5)\n",
        "b1 = np.random.randn(4,1)\n",
        "W2 = np.random.randn(3,4)\n",
        "b2 = np.random.randn(3,1)\n",
        "W3 = np.random.randn(1,3)\n",
        "b3 = np.random.randn(1,1)\n",
        "\n",
        "parameters = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "              \n",
        "AL, caches = L_model_forward(X, parameters)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AL = [[0.23196871 0.98007404 0.54577147 0.30385374]]\n",
            "Length of caches list = 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adSpJxsLQ-pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Slq39ClRNqF",
        "colab_type": "text"
      },
      "source": [
        "# Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ7Hhw76RhvL",
        "colab_type": "text"
      },
      "source": [
        "Using cross entropy loss equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Td-qG7JRcRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = Y.shape[1]\n",
        "    # cost function\n",
        "    cost = -1./m * np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
        "    # this turns [['size']] into 'size'\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1_gRygnSWTf",
        "colab_type": "code",
        "outputId": "eda89b6f-1246-42aa-f718-884029361e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test case\n",
        "Y = np.asarray([[1, 1, 0]])\n",
        "AL = np.array([[.8,.9,0.4]])\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost = 0.2797765635793422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2CDN8rJSm-H",
        "colab_type": "text"
      },
      "source": [
        "# Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udtva8wdTOsH",
        "colab_type": "text"
      },
      "source": [
        "2-layer network: linear backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKOVBx6CTSIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    # Calculate gradients\n",
        "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    # Asseert shapes\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp1xlHE7Ugcn",
        "colab_type": "code",
        "outputId": "0d75f9c2-2928-4fa5-df30-a5f7017297b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# test case\n",
        "np.random.seed(1)\n",
        "dZ = np.random.randn(3,4)\n",
        "A = np.random.randn(5,4)\n",
        "W = np.random.randn(3,5)\n",
        "b = np.random.randn(3,1)\n",
        "linear_cache = (A, W, b)\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
            " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
            " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
            " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
            " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
            "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
            " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
            " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
            "db = [[-0.14713786]\n",
            " [-0.11313155]\n",
            " [-0.13209101]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJKDaz-CU3-2",
        "colab_type": "text"
      },
      "source": [
        "2-layer network: linear activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UEaNHz1U5o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XophNqUVGGY",
        "colab_type": "code",
        "outputId": "6174abf9-fffc-496d-f10a-8268dda10350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# test case\n",
        "np.random.seed(2)\n",
        "dAL = np.random.randn(1,2)\n",
        "A = np.random.randn(3,2)\n",
        "W = np.random.randn(1,3)\n",
        "b = np.random.randn(1,1)\n",
        "Z = np.random.randn(1,2)\n",
        "linear_cache = (A, W, b)\n",
        "activation_cache = Z\n",
        "linear_activation_cache = (linear_cache, activation_cache)\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "\n",
        "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid:\n",
            "dA_prev = [[ 0.11017994  0.01105339]\n",
            " [ 0.09466817  0.00949723]\n",
            " [-0.05743092 -0.00576154]]\n",
            "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
            "db = [[-0.05729622]]\n",
            "\n",
            "relu:\n",
            "dA_prev = [[ 0.44090989  0.        ]\n",
            " [ 0.37883606  0.        ]\n",
            " [-0.2298228   0.        ]]\n",
            "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
            "db = [[-0.20837892]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnAM92lAVbbD",
        "colab_type": "text"
      },
      "source": [
        "L-layer model: backward linear + activation\n",
        "\n",
        "  - output -> Sigmoid -> Linear -> [RELU -> Linear (repeated)]\n",
        "  - find dAL from output of forward prop\n",
        "  - find dA, dW, dB for sigmoid layer\n",
        "  - repeat: find dA, dW, dB for RELU layer\n",
        "  - store all gradients in a dictionary for parameter updating\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGlND2TiViD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "\n",
        "    # first step\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    # Lth Layer: sigmoid\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "\n",
        "    # Loop for RELU -> Linear\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yDB7xdFXaHC",
        "colab_type": "code",
        "outputId": "555a064d-c5f2-482a-8c5e-a1df3e42c38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# test case\n",
        "np.random.seed(3)\n",
        "AL = np.random.randn(1, 2)\n",
        "Y_assess = np.array([[1, 0]])\n",
        "\n",
        "A1 = np.random.randn(4,2)\n",
        "W1 = np.random.randn(3,4)\n",
        "b1 = np.random.randn(3,1)\n",
        "Z1 = np.random.randn(3,2)\n",
        "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
        "\n",
        "A2 = np.random.randn(3,2)\n",
        "W2 = np.random.randn(1,3)\n",
        "b2 = np.random.randn(1,1)\n",
        "Z2 = np.random.randn(1,2)\n",
        "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
        "\n",
        "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
        "\n",
        "grads = L_model_backward(AL, Y_assess, caches)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dA1 = \"+ str(grads[\"dA1\"])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
            "db1 = [[-0.22007063]\n",
            " [ 0.        ]\n",
            " [-0.02835349]]\n",
            "dA1 = [[ 0.12913162 -0.44014127]\n",
            " [-0.14175655  0.48317296]\n",
            " [ 0.01663708 -0.05670698]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWTN1b3mXjxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdiL4FKfX8Ax",
        "colab_type": "text"
      },
      "source": [
        "# Update Paramaters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31M8GJ5JX_Nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2 \n",
        "\n",
        "    # update parameters\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrt4YvhiYSok",
        "colab_type": "code",
        "outputId": "f6d36710-f52c-4ac0-d7c9-8d8ffa86fe8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# test case\n",
        "np.random.seed(2)\n",
        "W1 = np.random.randn(3,4)\n",
        "b1 = np.random.randn(3,1)\n",
        "W2 = np.random.randn(1,3)\n",
        "b2 = np.random.randn(1,1)\n",
        "parameters = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2}\n",
        "np.random.seed(3)\n",
        "dW1 = np.random.randn(3,4)\n",
        "db1 = np.random.randn(3,1)\n",
        "dW2 = np.random.randn(1,3)\n",
        "db2 = np.random.randn(1,1)\n",
        "grads = {\"dW1\": dW1,\n",
        "          \"db1\": db1,\n",
        "          \"dW2\": dW2,\n",
        "          \"db2\": db2}\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
            " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
            " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
            "b1 = [[-0.04659241]\n",
            " [-1.28888275]\n",
            " [ 0.53405496]]\n",
            "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
            "b2 = [[-0.84610769]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku9Abp-JYZxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIigZ7_tZkkJ",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFikrwrbZqoa",
        "colab_type": "text"
      },
      "source": [
        "Implementing 2-layer network to classify images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpeiDsCxZ6Px",
        "colab_type": "text"
      },
      "source": [
        "Dataset: Cat vs. Non-Cat\n",
        "\n",
        "Provided with a data file 'data.h5' which contains:\n",
        "\n",
        "  - training set of m images that are labelled as cat or non-cat\n",
        "  - test set of images\n",
        "  - images are of shape: (pixels, pixels, 3) - 3 channels for RGB\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NNlg6C9jMAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VevV8fyD25CX",
        "colab_type": "code",
        "outputId": "75c4145a-5c6a-46ce-9bd9-ab58e5245e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYY2LQykAiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    train_dataset = h5py.File('/content/drive/My Drive/Deep Learning Coursera/Deep Neural Net - Cat Classifier/train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('/content/drive/My Drive/Deep Learning Coursera/Deep Neural Net - Cat Classifier/test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muImCREIltO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5e-Cn0MnK1o",
        "colab_type": "code",
        "outputId": "39fe38f1-4277-4868-bd34-4743581c0054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "# Example of a picture\n",
        "index = 2\n",
        "plt.imshow(train_x_orig[index])\n",
        "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 1. It's a cat picture.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uIfIMDy6sH2",
        "colab_type": "code",
        "outputId": "a34b2b0b-df26-40ae-cb53-a96827cc9035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Explore your dataset \n",
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "\n",
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
        "print (\"train_y shape: \" + str(train_y.shape))\n",
        "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
        "print (\"test_y shape: \" + str(test_y.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 209\n",
            "Number of testing examples: 50\n",
            "Each image is of size: (64, 64, 3)\n",
            "train_x_orig shape: (209, 64, 64, 3)\n",
            "train_y shape: (1, 209)\n",
            "test_x_orig shape: (50, 64, 64, 3)\n",
            "test_y shape: (1, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo43eROx7IrA",
        "colab_type": "code",
        "outputId": "aa0e3c4e-4259-4687-f906-72e273455454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Reshape the training and test examples \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (12288, 209)\n",
            "test_x's shape: (12288, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3nmKdBs7uGt",
        "colab_type": "text"
      },
      "source": [
        "Process to implement:\n",
        "\n",
        "  - Initialize parameters\n",
        "  - Loop for num_iterations\n",
        "    \n",
        "    - Forward Prop.\n",
        "    - Compute Cost\n",
        "    - Backward Prop.\n",
        "    - Update parameters.\n",
        "  \n",
        "  - Use trained parameters to predict labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTgacFQu8iv5",
        "colab_type": "text"
      },
      "source": [
        "2-layer Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKPSNagy8mwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CONSTANTS DEFINING THE MODEL ####\n",
        "n_x = 12288     # num_px * num_px * 3\n",
        "n_h = 7 # hidden layers\n",
        "n_y = 1 # output node\n",
        "layers_dims = (n_x, n_h, n_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9G_0o2e8v4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    costs = [] # to keep track of the cost\n",
        "    m = X.shape[1] # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "\n",
        "    # use previous initialization fucntion\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
        "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
        "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
        "        \n",
        "        # Compute cost\n",
        "        cost = compute_cost(A2, Y)\n",
        "        \n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "        \n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
        "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
        "        \n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "       \n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKEKbJbi-QMz",
        "colab_type": "code",
        "outputId": "c2e52030-2c5a-47f4-b726-b26973cb9699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "source": [
        "# test case\n",
        "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.6936890276336564\n",
            "Cost after iteration 100: 0.6479551712905036\n",
            "Cost after iteration 200: 0.6356911521977447\n",
            "Cost after iteration 300: 0.6080457268786562\n",
            "Cost after iteration 400: 0.5672335207560901\n",
            "Cost after iteration 500: 0.5255545368934091\n",
            "Cost after iteration 600: 0.4784230698515129\n",
            "Cost after iteration 700: 0.4201501902223851\n",
            "Cost after iteration 800: 0.39248973788351843\n",
            "Cost after iteration 900: 0.37904885764520413\n",
            "Cost after iteration 1000: 0.32985401173003703\n",
            "Cost after iteration 1100: 0.2948202517059938\n",
            "Cost after iteration 1200: 0.2483400112556411\n",
            "Cost after iteration 1300: 0.20169021751691749\n",
            "Cost after iteration 1400: 0.16989855048521438\n",
            "Cost after iteration 1500: 0.1353840213002415\n",
            "Cost after iteration 1600: 0.12087036266509116\n",
            "Cost after iteration 1700: 0.10374776808455388\n",
            "Cost after iteration 1800: 0.08625078735516983\n",
            "Cost after iteration 1900: 0.07387471899033579\n",
            "Cost after iteration 2000: 0.0630233487345374\n",
            "Cost after iteration 2100: 0.05561893155702896\n",
            "Cost after iteration 2200: 0.049379410498807605\n",
            "Cost after iteration 2300: 0.044092273669510025\n",
            "Cost after iteration 2400: 0.039703831938917696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxBG9sA6_E5i",
        "colab_type": "text"
      },
      "source": [
        "2-layer Model: Prediction method\n",
        "\n",
        "  - computing a forward pass (AL and cache) \n",
        "  - using the sigmoid outputs of AL to determine predictions\n",
        "  - iterate through all of AL.shape[1] and push results to 1 OR 0\n",
        "  - sum how many times p == y and divide by training examples 'm'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvou9GLO-WH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTkVJmfCJ-oo",
        "colab_type": "code",
        "outputId": "2e3a08ba-4d2e-461e-c8dc-a9172e7ad00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test the predict method on our model (Training data)\n",
        "predictions_train = predict(train_x, train_y, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8K1xOqZKED0",
        "colab_type": "code",
        "outputId": "a18b1780-6f03-4dcf-8ca6-713574edb893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test the predict method on our model (Testing data)\n",
        "predictions_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}